{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_path = '/home/jovyan/work/data'\n",
    "data_path = 'D:\\\\analise-airbnb-rentabilite\\data'\n",
    "df = pd.read_csv(os.path.join(data_path, 'listings_process.csv'), sep=',')\n",
    "df.head()"
   ],
   "id": "71dd4ea8ba8516a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot distribution of df['amenities'].unique()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "uniques = {}\n",
    "for i in df['amenities']:\n",
    "    for j in i.split(','):\n",
    "        if j in uniques:\n",
    "            uniques[j] += 1\n",
    "        else:\n",
    "            uniques[j] = 1\n",
    "print(f\"Uniques: {len(uniques)}\")\n",
    "# Keep only the top 5\n",
    "uniques = {k.replace(\" \", ''): v for k, v in sorted(uniques.items(), key=lambda item: item[1], reverse=True)[:5]}\n",
    "print(f\"Uniques: {len(uniques)}\")\n",
    "# plot with labels horizontaly\n",
    "plt.barh(list(uniques.keys()), list(uniques.values()))\n",
    "plt.show()\n",
    "print(uniques.keys())"
   ],
   "id": "f154410f7abdd784"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    'id', 'room_type', 'price', 'availability_365', 'longitude', 'latitude',\n",
    "       'property_type', 'room_type.1', 'accommodates', 'bathrooms', 'bedrooms',\n",
    "       'beds', 'amenities', 'availability_30', 'availability_60',\n",
    "       'availability_90', 'availability_365.1', 'number_of_rooms', 'surface',\n",
    "       'price_per_m2'\n",
    "\n",
    "    'Kitchen', 'Wifi', 'Smokealarm', 'Dishesandsilverware', 'Bedlinens', 'Refrigerator', 'Hairdryer', 'Hangers', 'Iron', 'Cookingbasics', 'Essentials', 'Microwave', 'Hotwaterkettle', 'Washer', 'Cleaningproducts', 'Wineglasses', 'Hotwater', 'Dedicatedworkspace', 'Shampoo', 'Heating'\n",
    "    \"\"\"\n",
    "    print(\"Start preprocess data\")\n",
    "    print(\"Drop columns\")\n",
    "    df = df.copy()\n",
    "    center_lat = df['latitude'].mean()\n",
    "    center_lon = df['longitude'].mean()\n",
    "    # Normalize latitude and longitude\n",
    "    df['latitude'] = df['latitude'] - center_lat\n",
    "    df['longitude'] = df['longitude'] - center_lon\n",
    "\n",
    "    df['latitude'] = df['latitude'] / df['latitude'].max()\n",
    "    df['longitude'] = df['longitude'] / df['longitude'].max()\n",
    "\n",
    "    df['number_of_rooms'] = np.clip(df['number_of_rooms'], 0, 10)\n",
    "    df['price'] = np.log1p(df['price'])\n",
    "    df['price'] = np.clip(df['price'], 3.2, 7)\n",
    "    # df['price'] = np.clip(df['price'], 10, 600)\n",
    "\n",
    "    X = df.drop(\n",
    "        ['id', 'room_type', 'room_type.1', 'property_type', 'availability_365.1', 'availability_30', 'availability_60',\n",
    "         'availability_90', 'availability_365', 'price_per_m2', 'surface'],\n",
    "        axis=1)\n",
    "    # amenities: split by ',', to hot encode\n",
    "    print(\"Process amenities\")\n",
    "\n",
    "    amenities_old = X['amenities']\n",
    "    # create a raw df with uniques.keys() as columns\n",
    "    amenities = pd.DataFrame()\n",
    "    for k in uniques.keys():\n",
    "        amenities[k] = amenities_old.apply(lambda x: 1 if k in x else 0)\n",
    "\n",
    "    print(\"Concat amenities\")\n",
    "    X = pd.concat([X, amenities], axis=1)\n",
    "    print(\"Drop amenities\")\n",
    "    X = X.drop(['amenities'], axis=1)\n",
    "\n",
    "    return X"
   ],
   "id": "6a75d31944f0f169"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "def evaluate_model(train, test, model_name, model_display_name, model_class, model_params):\n",
    "    t = time.time()\n",
    "    X_train, y_train = train.drop(['price'], axis=1), train['price']\n",
    "    X_test, y_test = test.drop(['price'], axis=1), test['price']\n",
    "    print(\"Instantiate model\")\n",
    "    model = model_class(**model_params)\n",
    "    print(\"Fit model\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Predict model\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Drawing results statistics\")\n",
    "    print(\"Accuracy score\")\n",
    "    print(f'{model_display_name} Accuracy: {model.score(X_test, y_test):.2f}')\n",
    "    print(\"Mean squared error\")\n",
    "    y_test = np.exp(y_test)\n",
    "    y_pred = np.exp(y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'{model_display_name} MSE: {mse:.2f}')\n",
    "    mm = y_test - y_pred\n",
    "    print(f\"Mean: {mm.mean()}\")\n",
    "    print(f\"Time: {time.time() - t}\")\n",
    "    # plot y-test vs y-pred\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.axis('equal')\n",
    "    plt.axis('square')\n",
    "    plt.xlim([0, plt.xlim()[1]])\n",
    "    plt.ylim([0, plt.ylim()[1]])\n",
    "    _ = plt.plot([0, 1000], [0, 1000])\n",
    "    plt.show()\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'mse': mse,\n",
    "        'model_params': model_params,\n",
    "    }"
   ],
   "id": "6e826ae41c84496d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "df1 = preprocess_data(df)"
   ],
   "id": "22312c557dff4f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Draw distribution of each column in df1\n",
    "df1.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ],
   "id": "142e45053015790c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df1_train, df1_test = train_test_split(df1, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(f\"Train shape: {df1_train.shape}\")\n",
    "print(f\"Test shape: {df1_test.shape}\")\n",
    "print(df1_train.head())"
   ],
   "id": "1df730ac125fd7df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(df1_train)",
   "id": "65b335d22c9313b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR\n",
    "\n",
    "model_params = {\n",
    "    'kernel': 'linear',\n",
    "    'C': 1.0,\n",
    "    'epsilon': 0.01\n",
    "}\n",
    "\n",
    "evaluate_model(df1_train, df1_test, 'svr', 'SVR', model, model_params)"
   ],
   "id": "2c1066e1e6fa913f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression\n",
    "\n",
    "model_params = {\n",
    "    'fit_intercept': True\n",
    "}\n",
    "\n",
    "evaluate_model(df1_train, df1_test, 'linear_regression', 'Linear Regression', model, model_params)"
   ],
   "id": "7444ad2100a58402"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Now using neural networks\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model = MLPRegressor\n",
    "\n",
    "model_params = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "evaluate_model(df1_train, df1_test, 'mlp', 'MLP', model, model_params)"
   ],
   "id": "5764be6d303ba17e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Mutli-layer neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier\n",
    "\n",
    "model_params = {\n",
    "\n",
    "    'hidden_layer_sizes': (100, 100, 50),\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "evaluate_model(df1_train, df1_test, 'mlp', 'MLP', model, model_params)"
   ],
   "id": "2426f01bcdd77b84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install tensorflow",
   "id": "f933679dff7d1754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X_train, y_train = df1_train.drop(['price'], axis=1), df1_train['price']\n",
    "X_test, y_test = df1_test.drop(['price'], axis=1), df1_test['price']\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.2),  # Add dropout for regularization\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with better loss and metrics\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Add callbacks for better training control\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ],
   "id": "1c4279e329dc7af5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.expm1(mse)",
   "id": "de3dc85f7290284d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(10):\n",
    "    model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=2, batch_size=32)"
   ],
   "id": "b14a9e5390e67b95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# GridSearchCV avec Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_log = best_model.predict(X_test)\n",
    "y_pred = np.exp(y_pred_log)  # Re-transformer les prédictions\n",
    "\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "print(\"MSE :\", mean_squared_error(np.exp(y_test), y_pred))\n"
   ],
   "id": "f9aa21fedfd6ed53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f9edbe0a5f0df0dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
